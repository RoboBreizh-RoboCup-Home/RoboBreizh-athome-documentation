# Gentoo Prefix
The first challenge of working with the Pepper Robot is the integrated NaoQi OS version 2.5.5, based on a 32 bits version of Linux Gentoo that restricted the number of libraries that could be installed. Also, Pepper's native API was written in Python2 and it does not include commonly used libraries in modern libraries, such as PyTorch and TensorFlow or Robot Operating System (ROS). The second limitation is the unavailability of root access to the robot's OS. To overcome those two limitations, we created a continuous integration pipeline. 
Even though Gentoo is an old system, it offers some interesting solutions such as the package manager *Portage* and the *Gentoo Prefix* Project. 

A Gentoo prefix is an offset version of Gentoo that could be installed alongside another OS without root permission. In practice, we use an integration pipeline based on a Docker Image of a 32 bits [Gentoo prefixe](https://github.com/awesomebytes/gentoo\_prefix\_ci\_32b) and then built it on top of our existing OS and it became our full architecture. 

At runtime, the required components of the prefix were extracted from the Docker Image and pushed directly to the robot, alongside the NaoQi OS.
This solution resolved the root access problem but the limitations of Gentoo (32 bits and the *Portage* package manager instead of *Aptitude*) remained. As a workaround, we decided to cross-compile libraries, including ROS, in Docker using a dump of the [Pepper OS](https://hub.docker.com/r/awesomebytes/pepper\_2.5.5.5) and our Gentoo prefix. We initially used the [ros-overlay project](https://github.com/ros/ros-overlay) to cross-compile ROS Noetic. 

Since Python3 in the native Python API from Pepper is not available, thus we built on top of the LibQi API a version that could run onboard and allowed us to still have access to the [resources of the robot](https://github.com/Maelic/libqi-python). As we also concern about computational speed and hardware optimisation, we decided to use dedicated inference engines such as Tensorflow Lite and [ONNX](https://github.com/onnx/onnx) cross-compiled for the Pepper CPU Intel ATOM as well as other ML tools such as [OpenCV4.6](https://github.com/itseez/opencv) and Kaldi running with Python3 on Pepper hardware. 